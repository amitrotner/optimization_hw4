#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
% Variables to change every new document
\newcommand{\hmwkClass}{236330 - Introduction to Optimization} % Course/class
\newcommand{\hmwkNumber}{4} % Homework number

% Constants to set once
\newcommand{\hmwkAuthorNameI}{Amit Rotner} % Your name
\newcommand{\hmwkStudentNumberI}{123456789} % Student number
\newcommand{\hmwkAuthorNameII}{Or Steiner} % Your name
\newcommand{\hmwkStudentNumberII}{123456789} % Student number

% Packages
\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

% General document properties
\linespread{1.1} % Line spacing
\setlength\parindent{0pt} % Removes all indentation from paragraphs

% Required to not count titlepage in page numbering
\addtocounter {page} {-1}

% Make a simple command for use in document body
\newcommand{\start}{
\maketitle
\thispagestyle{empty}
\newpage
}

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorNameI\ and \hmwkAuthorNameII} % Top left header
\rhead{\hmwkClass:\ Homework\ \#\hmwkNumber}
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

% The following 2 commands setup the title page
\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \\ Homework\ \#\hmwkNumber}}\\
\normalsize\vspace{0.1in}\small{ \today }\\
\vspace{3in}
}

\author{
  \textbf{\hmwkAuthorNameI} \\
  \texttt{\hmwkStudentNumberI} \\
	\textbf{\hmwkAuthorNameII} \\
  \texttt{\hmwkStudentNumberII}
}

% Do not display standard date since we use a custom date
\date{}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip bigskip
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
start
\end_layout

\end_inset


\end_layout

\begin_layout Section*
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset

DNNs, Augmented Lagrangian and Constrained Optimization
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Part 1 – Deep Neural Network:
\end_layout

\begin_layout Paragraph*
Task 1:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f\left(x_{1},x_{2}\right)=x_{1}e^{-\left(x_{1}^{2}+x_{1}^{2}\right)}\ ,\phi=tanh\left(x\right)=\frac{1-e^{x}}{1+e^{x}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left\langle A,B\right\rangle =Tr\left(A^{T}B\right)=Tr\left(AB^{T}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
F\left(x,W_{1},W_{2},W_{3},b_{1},b_{2},b_{3}\right)=W_{3}^{T}\phi_{2}\left(W_{2}^{T}\phi_{1}\left(W_{1}^{T}x+b_{1}\right)+b_{2}\right)+b_{3}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
r_{i}=F\left(x^{i},W\right)-y_{i}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\psi\left(r\right)=r^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{r}\psi=2r
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E=\psi(r)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{x}E=J_{F\left(x\right)}^{T}\nabla_{r}\psi
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J_{F\left(x\right)}=W_{3}^{T}\Phi_{2}^{\prime}W_{2}^{T}\Phi_{1}^{\prime}W_{1}^{T}
\]

\end_inset


\end_layout

\begin_layout Standard
Let us denote
\begin_inset Formula $u_{1}=W_{1}^{T}x+b_{1}$
\end_inset

 and 
\begin_inset Formula $du_{1}=dW_{1}^{T}x$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
dE=\nabla_{u_{1}}E^{T}du_{1}=\nabla_{u_{1}}E^{T}dW_{1}^{T}x=Tr\left(\nabla_{u_{1}}E^{T}dW_{1}^{T}x\right)=Tr\left(x\nabla_{u_{1}}E^{T}dW_{1}^{T}\right)=\left\langle x\text{\nabla}_{u_{1}}E^{T},dW_{1}\right\rangle 
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{u_{1}}E=\Phi_{1}^{\prime}W_{2}\Phi_{2}^{\prime}W_{3}\nabla_{r}\psi
\]

\end_inset


\end_layout

\begin_layout Standard
And therefore the gradient of the Error with respect to 
\begin_inset Formula $W_{1}$
\end_inset

 is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{W_{1}}E=x\nabla_{r}\psi W_{3}^{T}\Phi_{2}^{\prime}W_{2}^{T}\Phi_{1}^{\prime}
\]

\end_inset


\end_layout

\begin_layout Standard
Let us denote 
\begin_inset Formula $u_{1}=W_{1}^{T}x+b_{1}$
\end_inset

 and 
\begin_inset Formula $du_{1}=db_{1}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
dE=\nabla_{u_{1}}E^{T}du_{1}=\nabla_{u_{1}}E^{T}db_{1}=Tr\left(\nabla_{u_{1}}E^{T}db_{1}\right)=\left\langle \nabla_{u_{1}}E,db_{1}\right\rangle 
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{u_{1}}E=\Phi_{1}^{\prime}W_{2}\Phi_{2}^{\prime}W_{3}\nabla_{r}\psi
\]

\end_inset


\end_layout

\begin_layout Standard
And therefore the gradient of the Error with respect to 
\begin_inset Formula $b_{1}$
\end_inset

 is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{b_{1}}E=\Phi_{1}^{\prime}W_{2}\Phi_{2}^{\prime}W_{3}\nabla_{r}\psi
\]

\end_inset


\end_layout

\begin_layout Standard
Let us denote 
\begin_inset Formula $u_{2}=W_{2}^{T}\phi_{1}\left(W_{1}^{T}x+b_{1}\right)+b_{2}$
\end_inset

 and 
\begin_inset Formula $du_{2}=dW_{2}^{T}\phi_{1}\left(u_{1}\right)$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
\begin_inset Formula 
\[
dE=\nabla_{u_{2}}E^{T}du_{2}=\nabla_{u_{2}}E^{T}dW_{2}^{T}\phi_{1}\left(u_{1}\right)=Tr\left(\nabla_{u_{2}}E^{T}dW_{2}^{T}\phi_{1}\left(u_{1}\right)\right)=Tr\left(\phi_{1}\left(u_{1}\right)\nabla_{u_{2}}E^{T}dW_{2}^{T}\right)=\left\langle ϕ_{1}u_{1}\nabla_{u_{2}}E^{T},dW_{2}\right\rangle 
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{u_{2}}E=\Phi_{2}^{\prime}W_{3}\nabla_{r}\psi
\]

\end_inset


\end_layout

\begin_layout Standard
And therefore the gradient of the Error with respect to 
\begin_inset Formula $W_{2}$
\end_inset

 is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{W_{2}}E=\phi_{1}\left(u_{1}\right)\nabla_{r}\psi W_{3}^{T}\Phi_{2}^{\prime}
\]

\end_inset


\end_layout

\begin_layout Standard
Let us denote 
\begin_inset Formula $u_{2}=W_{2}^{T}\phi_{1}\left(W_{1}^{T}x+b_{1}\right)+b_{2}$
\end_inset

 and 
\begin_inset Formula $du_{2}=db_{2}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
dE=\nabla_{u_{2}}E^{T}du_{2}=\nabla_{u_{2}}E^{T}db_{2}=Tr\left(\nabla_{u_{2}}E^{T}db_{2}\right)=Tr\left(\nabla_{u_{2}}E^{T}db_{2}\right)=\left\langle \nabla_{u_{2}}E,db_{2}\right\rangle 
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{u_{2}}E=\Phi_{2}^{\prime}W_{3}\nabla_{r}\psi
\]

\end_inset

And therefore the gradient of the Error with respect to 
\begin_inset Formula $b_{2}$
\end_inset

 is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{b_{2}}E=\Phi_{2}^{\prime}W_{3}\nabla_{r}\psi
\]

\end_inset


\end_layout

\begin_layout Standard
Let us denote 
\begin_inset Formula $u_{3}=W_{3}^{T}\phi_{2}\left(W_{2}^{T}\phi_{1}\left(W_{1}^{T}x+b_{1}\right)+b_{2}\right)+b_{3}$
\end_inset

 and 
\begin_inset Formula $du_{3}=dW_{3}^{T}\phi_{2}\left(u_{2}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
dE=\nabla_{u_{3}}E^{T}du_{3}=\nabla_{u_{3}}E^{T}dW_{3}^{T}\phi_{2}\left(u_{2}\right)=Tr\left(\nabla_{u_{2}}E^{T}dW_{3}^{T}\phi_{2}\left(u_{2}\right)\right)=Tr\left(\phi_{2}\left(u_{2}\right)\nabla_{u_{3}}E^{T}dW_{3}^{T}\right)=\left\langle ϕ_{2}u_{2}\nabla_{u_{3}}E^{T},dW_{3}\right\rangle 
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{u_{3}}E=\nabla_{r}\psi
\]

\end_inset


\end_layout

\begin_layout Standard
And therefore the gradient of the Error with respect to 
\begin_inset Formula $W_{3}$
\end_inset

 is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{W_{3}}E=\phi_{2}\left(u_{2}\right)\nabla_{r}\psi
\]

\end_inset


\end_layout

\begin_layout Standard
Let us denote
\begin_inset Formula $u_{3}=W_{3}^{T}\phi_{2}\left(W_{2}^{T}\phi_{1}\left(W_{1}^{T}x+b_{1}\right)+b_{2}\right)+b_{3}$
\end_inset

 and 
\begin_inset Formula $du_{3}=db_{3}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
dE=\nabla_{u_{3}}E^{T}du_{3}=\nabla_{u_{3}}E^{T}db_{3}=Tr\left(\nabla_{u_{2}}E^{T}db_{3}\right)=Tr\left(\nabla_{u_{3}}E^{T}db_{3}\right)=\left\langle \nabla_{u_{3}}E,db_{3}\right\rangle 
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{u_{3}}E=\nabla_{r}\psi
\]

\end_inset


\end_layout

\begin_layout Standard
And therefore the gradient of the Error with respect to 
\begin_inset Formula $b_{3}$
\end_inset

 is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{b_{3}}E=\nabla_{r}\psi
\]

\end_inset


\end_layout

\begin_layout Paragraph*
Task 2:
\end_layout

\begin_layout Standard
Plotting the Objective Function yields:
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename graphs/original_graph.svg
	scale 50

\end_inset


\end_layout

\begin_layout Paragraph*
Task 3:
\end_layout

\begin_layout Standard
Plotting the network reconstruction yields:
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename graphs/reconstructed_graph.svg
	scale 25

\end_inset


\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename graphs/convergence_graph.svg
	scale 25

\end_inset


\end_layout

\begin_layout Subsubsection*
Part 2 – Augmented Lagrangian:
\end_layout

\begin_layout Paragraph*
Task 1:
\end_layout

\begin_layout Standard
Consider the following quadratic problem:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\underset{_{x}}{min}\ 2\left(x_{1}-5\right)^{2}+\left(x_{2}-1\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
s.t.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
x_{2}\le & 1-\frac{x_{1}}{2}\\
x_{2}\ge & x_{1}\\
x_{2}\ge & -x_{1}
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
Is this problem convex?
\end_layout

\begin_layout Enumerate
Find the optimal solution 
\begin_inset Formula $\left(x^{*},\lambda^{*}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
Calculate the Lagrange multipliers using KKT conditions.
 What is the optimal value of the objective function?
\end_layout

\begin_layout Paragraph*
Solution:
\end_layout

\begin_layout Enumerate
\noindent
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 0
inner_pos "t"
use_parbox 0
use_makebox 0
width ""
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/feasible area and level sets.svg
	scale 10

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The feasible area and the level sets of the objective function
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Therefore the active constraints are 
\begin_inset Formula $c_{1}$
\end_inset

 and 
\begin_inset Formula $c_{2}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Enumerate
It is fairly clear from the figure above that the solution is 
\begin_inset Formula $x^{*}=\left(\frac{2}{3},\frac{2}{3}\right)^{T}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Writing the Lagrangian yields:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{L}\left(x,\lambda\right)=2\left(x_{1}-5\right)^{2}+\left(x_{2}-1\right)^{2}+\lambda_{1}\left(x_{2}-1+\frac{x_{1}}{2}\right)+\lambda_{2}\left(x_{1}-x_{2}\right)+\lambda_{3}\left(-x_{1}-x_{2}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
First we find the gradient of the Lagrangian:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{x}\mathcal{L}\left(x,\lambda\right)=\begin{pmatrix}4\left(x_{1}-5\right)+0.5\lambda_{1}+\lambda_{2}-\lambda_{3}, & 2\left(x_{2}-1\right)+\lambda_{1}-\lambda_{2}-\lambda_{3}\end{pmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $x^{*}$
\end_inset

 is an optimal solution for our problem, then there exist 
\begin_inset Formula $\lambda^{*}$
\end_inset

 which satisfies the KKT conditions: 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\nabla_{x_{1}}\mathcal{L}\left(x^{*},\lambda\right)=0\Rightarrow4\left(x_{1}-5\right)+0.5\lambda_{1}+\lambda_{2}-\lambda_{3}=0$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\nabla_{x_{2}}\mathcal{L}\left(x^{*},\lambda\right)=0\Rightarrow2\left(x_{2}-1\right)+\lambda_{1}-\lambda_{2}-\lambda_{3}=0$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $x_{2}-1+\frac{x_{1}}{2}\le0$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $x_{1}-x_{2}\le0\Rightarrow x_{1}\le x_{2}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $-x_{1}-x_{2}\le0\Rightarrow-x_{2}\le x_{1}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\lambda^{*}\ge0$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\lambda_{1}\left(x_{2}-1+\frac{x_{1}}{2}\right)=0$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\lambda_{2}\left(x_{1}-x_{2}\right)=0$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\lambda_{3}\left(-x_{1}-x_{2}\right)=0$
\end_inset


\end_layout

\begin_layout Standard
From 
\begin_inset Formula $\left(9\right)$
\end_inset

 we get: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\lambda_{3}=0
\]

\end_inset


\end_layout

\begin_layout Standard
From 
\begin_inset Formula $\left(1\right)+\left(2\right)$
\end_inset

 we get:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
0.5\lambda_{1}+\lambda_{2} & =17\frac{1}{3}\\
\lambda_{1}-\lambda_{2} & =\frac{2}{3}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Therefore, we get 
\begin_inset Formula $\lambda_{1}=12,\lambda_{2}=11\frac{1}{3}$
\end_inset

.
\end_layout

\begin_layout Standard
In conclusion:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x^{*}=\left(\frac{2}{3},\frac{2}{3}\right)^{T},\ \lambda^{*}=\left(12,11\frac{1}{3},0\right)^{T}
\]

\end_inset


\end_layout

\begin_layout Standard
Hence, the optimal value of the objective function is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
2\left(\frac{2}{3}-5\right)^{2}+\left(\frac{2}{3}-1\right)^{2}=37\frac{2}{3}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
From the above, we know that:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{L}\left(x,\lambda\right)=2\left(x_{1}-5\right)^{2}+\left(x_{2}-1\right)^{2}+\lambda_{1}\left(x_{2}-1+\frac{x_{1}}{2}\right)+\lambda_{2}\left(x_{1}-x_{2}\right)+\lambda_{3}\left(-x_{1}-x_{2}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
If we hold 
\begin_inset Formula $\lambda_{1},\lambda_{2},\lambda_{3}$
\end_inset

 fixed, this is a convex function of 
\begin_inset Formula $\left(x_{1},x_{2}\right)^{T}$
\end_inset

 .
\end_layout

\begin_layout Standard
Therefore, the infimum with respect to 
\begin_inset Formula $\left(x_{1},x_{2}\right)^{T}$
\end_inset

 is achieved when the partial derivatives 
\end_layout

\begin_layout Standard
with respect to 
\begin_inset Formula $x_{1}$
\end_inset

 and 
\begin_inset Formula $x_{2}$
\end_inset

 are zero, that is,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
4\left(x_{1}-5\right)+0.5\lambda_{1}+\lambda_{2}-\lambda_{3}=0
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
2\left(x_{2}-1\right)+\lambda_{1}-\lambda_{2}-\lambda_{3}=0
\]

\end_inset


\end_layout

\begin_layout Standard
Hence,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x_{1}=\frac{20-0.5\lambda_{1}-\lambda_{2}+\lambda_{3}}{4}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x_{2}=\frac{2-\lambda_{1}+\lambda_{2}+\lambda_{3}}{2}
\]

\end_inset


\end_layout

\begin_layout Standard
By substituting these infimal values into 
\begin_inset Formula $\mathcal{L}\left(x,\lambda\right)$
\end_inset

 we obtain the dual objective:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\mathcal{L}\left(x,\lambda\right) & =2\left(x_{1}-5\right)^{2}+\left(x_{2}-1\right)^{2}+\lambda_{1}\left(x_{2}-1+\frac{x_{1}}{2}\right)+\lambda_{2}\left(x_{1}-x_{2}\right)+\lambda_{3}\left(-x_{1}-x_{2}\right)\\
 & =2\left(\frac{20-0.5\lambda_{1}-\lambda_{2}+\lambda_{3}}{4}-5\right)^{2}+\left(\frac{2-\lambda_{1}+\lambda_{2}+\lambda_{3}}{2}-1\right)^{2}\\
 & +\lambda_{1}\left(\frac{2-\lambda_{1}+\lambda_{2}+\lambda_{3}}{2}-1+\frac{20-0.5\lambda_{1}-\lambda_{2}+\lambda_{3}}{8}\right)+\lambda_{2}\left(\frac{20-0.5\lambda_{1}-\lambda_{2}+\lambda_{3}}{4}-\frac{2-\lambda_{1}+\lambda_{2}+\lambda_{3}}{2}\right)\\
 & +\lambda_{3}\left(-\frac{20-0.5\lambda_{1}-\lambda_{2}+\lambda_{3}}{4}-\frac{2-\lambda_{1}+\lambda_{2}+\lambda_{3}}{2}\right)
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Hence, the dual problem is:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\underset{_{\lambda\ge0}}{max} & 2\left(\frac{20-0.5\lambda_{1}-\lambda_{2}+\lambda_{3}}{4}-5\right)^{2}+\left(\frac{2-\lambda_{1}+\lambda_{2}+\lambda_{3}}{2}-1\right)^{2}\\
 & +\lambda_{1}\left(\frac{2-\lambda_{1}+\lambda_{2}+\lambda_{3}}{2}-1+\frac{20-0.5\lambda_{1}-\lambda_{2}+\lambda_{3}}{8}\right)+\lambda_{2}\left(\frac{20-0.5\lambda_{1}-\lambda_{2}+\lambda_{3}}{4}-\frac{2-\lambda_{1}+\lambda_{2}+\lambda_{3}}{2}\right)\\
 & +\lambda_{3}\left(-\frac{20-0.5\lambda_{1}-\lambda_{2}+\lambda_{3}}{4}-\frac{2-\lambda_{1}+\lambda_{2}+\lambda_{3}}{2}\right)
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Substitute the Lagrange multipliers from section 
\begin_inset Formula $3$
\end_inset

 into the dual problem, yields that the value 
\end_layout

\begin_deeper
\begin_layout Standard
of the dual problem is 
\begin_inset Formula $=37\frac{2}{3}$
\end_inset

 which it also the optimum of the dual problem.
 
\end_layout

\begin_layout Standard
Since it is also the optimal value of the primal problem, the duality gap
 is zero.
\end_layout

\end_deeper
\begin_layout Paragraph*
Task 2:
\end_layout

\begin_layout Standard
Writing the quadratic programming problem used in Task 1 in general form,
 yields:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{1}{2}\begin{pmatrix}x_{1} & x_{2}\end{pmatrix}\underset{Q}{\underbrace{\begin{pmatrix}4 & 0\\
0 & 2
\end{pmatrix}}}\begin{pmatrix}x_{1}\\
x_{2}
\end{pmatrix}+\underset{d^{T}}{\underbrace{\begin{pmatrix}-20 & -2\end{pmatrix}}}\begin{pmatrix}x_{1}\\
x_{2}
\end{pmatrix}+\underset{e}{\underbrace{51}}
\]

\end_inset


\end_layout

\begin_layout Standard
s.t.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\underset{A}{\underbrace{\begin{pmatrix}0.5 & 1\\
1 & -1\\
-1 & -1
\end{pmatrix}}}\begin{pmatrix}x_{1}\\
x_{2}
\end{pmatrix}-\underset{b}{\underbrace{\begin{pmatrix}1\\
0\\
0
\end{pmatrix}}}\le0
\]

\end_inset


\end_layout

\begin_layout Paragraph*
Using the Augmented Lagrangian Solver to Solve a Quadratic Programming Problem:
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Graphics
	filename graphs/part2_wet.png
	scale 50

\end_inset


\end_layout

\begin_layout Subsubsection*
Part 3 – Constrained Optimization:
\end_layout

\begin_layout Paragraph*
Task 1:
\end_layout

\begin_layout Standard
Consider the following optimization problem:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\underset{_{x}}{min}\ x^{T}Mx+c^{T}x
\]

\end_inset


\end_layout

\begin_layout Standard
s.t.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
Ax & =b\\
M & \succ0\\
A\in\mathbb{R}^{m\times n},x\in\mathbb{R}^{n},c\in\mathbb{R}^{n} & ,b\in\mathbb{R}^{n},M\in\mathbb{R}^{m\times m}
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
Is this problem convex?
\end_layout

\begin_layout Enumerate
Find the optimal solution 
\begin_inset Formula $\left(x^{*},\lambda^{*}\right)$
\end_inset


\end_layout

\begin_layout Paragraph*
Solution:
\end_layout

\begin_layout Enumerate
We start by finding the Hessian of the objective function:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{align*}
df & =dx^{T}Mx+x^{T}Mdx+c^{T}dx\\
 & \underset{_{\left(i\right)}}{=}x^{T}M^{T}dx+x^{T}Mdx+c^{T}dx\\
 & \underset{_{\left(ii\right)}}{=}2x^{T}Mdx+c^{T}dx\\
 & =\left(2x^{T}M+c^{T}\right)dx\\
 & =\left(2Mx+c\right)^{T}dx
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\left(i\right)$
\end_inset

 - Since 
\begin_inset Formula $dx^{T}Mx$
\end_inset

 is a scalar.
\end_layout

\begin_layout Standard
\begin_inset Formula $\left(ii\right)$
\end_inset

 - Since 
\begin_inset Formula $M=M^{T}.$
\end_inset


\end_layout

\begin_layout Standard
And therefore: 
\begin_inset Formula $\nabla f=2Mx+c$
\end_inset


\end_layout

\begin_layout Standard
Now, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
d\nabla f=2Mdx
\]

\end_inset


\end_layout

\begin_layout Standard
Hence, 
\begin_inset Formula $H=2M$
\end_inset

.
\end_layout

\begin_layout Standard
We know that 
\begin_inset Formula $M\succ0$
\end_inset

 and therefore 
\begin_inset Formula $H=2M$
\end_inset

 is also 
\begin_inset Formula $PD$
\end_inset

 and the problem is convex.
\end_layout

\end_deeper
\begin_layout Enumerate
Writing the Lagrangian yields:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{L}\left(x,\lambda\right)=f\left(x\right)+\lambda\left(Ax-b\right)
\]

\end_inset


\end_layout

\begin_layout Standard
First we find the gradient of the Lagrangian:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{x}\mathcal{L}\left(x,\lambda\right)=2Mx+c+\lambda A^{T}
\]

\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $x^{*}$
\end_inset

 is an optimal solution for our problem, then there exist 
\begin_inset Formula $\lambda^{*}$
\end_inset

 which satisfies the KKT conditions: 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\nabla_{x}\mathcal{L}\left(x,\lambda\right)=0\Rightarrow2Mx^{*}+\lambda A^{T}=-c$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $Ax^{*}-b=0$
\end_inset


\end_layout

\begin_layout Standard
Writing the above as a linear system:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{pmatrix}2M & A^{T}\\
A & 0
\end{pmatrix}\begin{pmatrix}x^{*}\\
\lambda^{*}
\end{pmatrix}=\begin{pmatrix}-c\\
b
\end{pmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
Assuming 
\begin_inset Formula $A$
\end_inset

 is invertible and since 
\begin_inset Formula $M$
\end_inset

 is PD, then 
\begin_inset Formula $\begin{pmatrix}2M & A^{T}\\
A & 0
\end{pmatrix}$
\end_inset

 is also invertible and:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{pmatrix}x^{*}\\
\lambda^{*}
\end{pmatrix}=\begin{pmatrix}2M & A^{T}\\
A & 0
\end{pmatrix}^{-1}\begin{pmatrix}-c\\
b
\end{pmatrix}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Paragraph*
Task 2:
\end_layout

\begin_layout Standard
Consider the following optimization problem:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\underset{_{x}}{min}\ \left\Vert x-c\right\Vert _{2}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
s.t.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
Ax & =b\\
A\in\mathbb{R}^{m\times n},x\in\mathbb{R}^{n},c\in\mathbb{R}^{n} & ,b\in\mathbb{R}^{n}
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
Is this problem convex?
\end_layout

\begin_layout Enumerate
Find the optimal solution 
\begin_inset Formula $\left(x^{*},\lambda^{*}\right)$
\end_inset


\end_layout

\begin_layout Paragraph*
Solution:
\end_layout

\begin_layout Enumerate
We start by proving that the objective function is convex:
\end_layout

\begin_deeper
\begin_layout Standard
We have seen in class that the Euclidean norm 
\begin_inset Formula $\left\Vert \cdot\right\Vert _{2}$
\end_inset

 is convex and therefore 
\begin_inset Formula $h\left(x\right)=\left\Vert x-c\right\Vert _{2}$
\end_inset

 is also convex.
\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $g\left(x\right)=x^{2}$
\end_inset

 is convex as well, and also it is non-decreasing on 
\begin_inset Formula $[0,∞)$
\end_inset

, the range of 
\begin_inset Formula $h$
\end_inset

, 
\end_layout

\begin_layout Standard
the composition 
\begin_inset Formula $f=f\circ h=\left\Vert x-c\right\Vert _{2}^{2}$
\end_inset

 is convex.
\end_layout

\begin_layout Standard
Hence, the problem is convex.
\end_layout

\end_deeper
\begin_layout Enumerate
Writing the Lagrangian yields:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{L}\left(x,\lambda\right)=f\left(x\right)+\lambda\left(Ax-b\right)
\]

\end_inset


\end_layout

\begin_layout Standard
First we find the gradient of the Lagrangian:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{x}\mathcal{L}\left(x,\lambda\right)=2x^{T}x-2x^{T}c+\lambda A^{T}
\]

\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $x^{*}$
\end_inset

 is an optimal solution for our problem, then there exist 
\begin_inset Formula $\lambda^{*}$
\end_inset

 which satisfies the KKT conditions: 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\nabla_{x}\mathcal{L}\left(x,\lambda\right)=0\Rightarrow2x^{*T}x^{*}-2x^{*T}c=\lambda A^{T}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $Ax^{*}-b=0$
\end_inset


\end_layout

\begin_layout Standard
Assuming 
\begin_inset Formula $A$
\end_inset

 is invertible, then 
\begin_inset Formula $x^{*}=A^{-1}b$
\end_inset

.
\end_layout

\begin_layout Standard
Hence, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
2b^{T}\left(A^{-1}\right)^{T}A^{-1}b-2b^{T}\left(A^{-1}\right)^{T}c=\lambda A^{T}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\lambda^{*}=2b^{T}\left(AA^{T}\right)^{-1}b\left(A^{T}\right)^{-1}-2b^{T}\left(A^{-1}\right)^{T}c\left(A^{T}\right)^{-1}
\]

\end_inset


\end_layout

\begin_layout Standard
Therefore, the optimal solution is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left(x^{*},\lambda^{*}\right)=\left(A^{-1}b,2b^{T}\left(AA^{T}\right)^{-1}b\left(A^{T}\right)^{-1}-2b^{T}\left(A^{-1}\right)^{T}c\left(A^{T}\right)^{-1}\right)
\]

\end_inset


\end_layout

\end_deeper
\end_body
\end_document
